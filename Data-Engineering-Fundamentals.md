# Data Engineering Fundamentals
The rise of `ML` in recent years is tightly coupled with the rise of big data. Large data systems, even without ML, are complex.


## **Data Sources**
An ML system can work with data from many different sources. They have different
characteristics, can be used for different purposes, and require different processing
methods. Understanding the sources your data comes from can help you use your
data more efficiently. 

1 **`User input data source`:** data explicitly input by users. User input can be
text, images, videos, uploaded files, etc. In most cases, when we input data, we
expect to get results back immediately. Therefore, user input data tends to require fast
processing.

2 **`system-generated data source`:** This is the data generated by different
components of your systems, which include various types of logs and system outputs
such as model predictions. Logs can record the state and significant events of the system, such as memory usage, number of instances, services called, packages used, etc. They can record the results
of different jobs, including large batch jobs for data processing and model training.

Because debugging ML systems is hard, it’s a common practice to log everything you
can. This means that your volume of logs can grow very, very quickly. This leads to
two problems. The first is that it can be hard to know where to look because signals
are lost in the noise. There have been many services that process and analyze logs,
such as Logstash, Datadog, Logz.io, etc. Many of them use ML models to help you
process and make sense of your massive number of logs.

The system also generates data to record users’ behaviors, such as clicking, 
choosing a suggestion, scrolling, zooming, ignoring a pop-up, or spending an 
unusual amount of time on certain pages. Even though this is system-generated data, 
it’s still considered part of user data and might be subject to privacy regulations.




## **Data Formats**
Once you have data, you might want to store it (or “persist” it, in technical terms).
Since your data comes from multiple sources with different access patterns,6 storing
your data isn’t always straightforward and, for some cases, can be costly. It’s important
to think about how the data will be used in the future so that the format you use will
make sense.

Here are some of the questions you might want to consider:
- How do I store multimodal data, e.g., a sample that might contain both images
and texts?
- Where do I store my data so that it’s cheap and still fast to access?
- How do I store complex models so that they can be loaded and run correctly on different hardware?

The process of converting a data structure or object state into a format that can be
stored or transmitted and reconstructed later is `data serialization`.


1. **JSON** 
`JSON`, JavaScript Object Notation, is everywhere. Even though it was derived from
JavaScript, it’s language-independent—most modern programming languages can
generate and parse `JSON`.  It’s human-readable. Its key-value pair paradigm is simple
but powerful, capable of handling data of different levels of structuredness.

example of the json structured format
```json
{
"firstName": "Boatie",
"lastName": "McBoatFace",
"isVibing": true,
"age": 12,
"address": {
"streetAddress": "12 Ocean Drive",
"city": "Port Royal",
"postalCode": "10021-3100"
}
}
```

json unstructured blob of text format
```json
{
"text": "Boatie McBoatFace, aged 12, is vibing, at 12 Ocean Drive, Port Royal,
10021-3100"
}
```


2. **Row-Major Versus Column-Major Format**
The two formats that are common and represent two distinct paradigms are `CSV` and `Parquet`. `CSV` (comma-separated values) is row-major, which means consecutive elements in a row are stored next to each other in memory. `Parquet` is column-major, which means consecutive elements in a column are stored next to each other.

Because modern computers process sequential data more efficiently than nonsequen‐
tial data, if a table is row-major, accessing its rows will be faster than accessing its
columns in expectation. This means that for row-major formats, accessing data by
rows is expected to be faster than accessing data by columns.

Row-major formats allow faster data writes. Consider the situation when you have to
keep adding new individual examples to your data. For each individual example, it’d
be much faster to write it to a file where your data is already in a row-major format.

Overall, row-major formats are better when you have to do a lot of writes, whereas
column-major ones are better when you have to do a lot of column-based reads.



3. **Text Versus Binary Format**
`CSV` and `JSON` are text files, whereas `Parquet` files are binary files. Text files are
files that are in plain text, which usually means they are human-readable. Binary
files are the catchall that refers to all nontext files. As the name suggests, binary files
are typically files that contain only `0s` and `1s`, and are meant to be read or used by
programs that know how to interpret the raw bytes.

AWS recommends using the Parquet format because “the Parquet format is up to 2x
faster to unload and consumes up to 6x less storage in Amazon S3, compared to text
formats.




## **Data Models**
Data models describe how data is represented. How you choose to represent data not only affects the way your systems are built, but also the problems your systems can solve. 


1. **Relational Model**
Relational models are among the most persistent ideas in computer science. In this model, data is organ‐
ized into relations; each relation is a set of tuples. A table is an accepted visual representation of a relation, and each row of a table makes up a tuple. Data following the relational model is usually stored in file formats like CSV or Parquet.

It’s often desirable for relations to be normalized. Data normalization can follow normal forms such as the `first normal form (1NF)`, `second normal form (2NF)`, etc.

One major downside of normalization is that your data is now spread across multiple
relations. You can join the data from different relations back together, but joining can
be expensive for large tables.

Figuring out how to execute an arbitrary query is the hard part, which is the job
of query optimizers. A query optimizer examines all possible ways to execute a
query and finds the fastest way to do so.


    Query optimization is the process of improving the efficiency of database queries by refining them to reduce execution time, minimize resource consumption, and enhance overall system performance. This involves techniques like optimizing join operations, minimizing unnecessary data retrieval, and using resources more efficiently. 


**From Declarative Data Systems to Declarative ML Systems**'
With a declarative ML system, users only need to declare the features’ schema and the task, and the system will figure out the best model to perform that task with the given features. Users won’t have to write code
to construct, train, and tune models. Declarative ML systems today abstract away the
model development part, which is the hard part that lies in feature engineering, data processing, model evaluation, data shift detection, continual learning, and so on. 




2. **NoSQL** 









